{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "67g7ZoAO3ZGc"
      },
      "outputs": [],
      "source": [
        "# Group - 14\n",
        "# Department of Computer Science, University at Buffalo\n",
        "\n",
        "# Harshitha\n",
        "\n",
        "# PS-2: Sentiment Analysis\n",
        "# Data Set - IMDB movie reviews - Kaggle\n",
        "# Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "WMJeqS86MDE7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset,random_split, RandomSampler,SequentialSampler\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,f1_score,recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7GzAecu4QCN",
        "outputId": "6e7fbdf5-7b13-4234-ca1d-812f5a9b938a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "#installing transformers, this step needs to be done when using colab\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9lRq4EqMS0C",
        "outputId": "3b75cd73-6c88-4756-de00-6c86f2532ce9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-82-e909ef7c1d6f>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df=pd.read_csv('IMDB Dataset.csv',error_bad_lines=False,engine=\"python\")\n"
          ]
        }
      ],
      "source": [
        "#used error_bad_lines =False because when reading data in colab errors are coming saying character 'c' or few characters are not found etc.\n",
        "df=pd.read_csv('IMDB Dataset.csv',error_bad_lines=False,engine=\"python\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "89WbuVm_Mubl"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import InputExample, InputFeatures\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "#tokenizer of bert-base model\n",
        "note = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmOfLKs-gG8D",
        "outputId": "ea8912de-b6f4-4c8b-d5d1-8ee526618569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n"
          ]
        }
      ],
      "source": [
        "print(df.head())\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "kMcxufdBlwCt"
      },
      "outputs": [],
      "source": [
        "sen_update={'positive':0, 'negative':1}\n",
        "df['sentiment'] = df['sentiment'].replace(sen_update)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B_z_fTznj44",
        "outputId": "917050bf-962f-4b73-e02a-a26d0f5eeba8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['review'].isnull().sum(axis=0) #0 implies no null values in any row of review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "JZNCWl4QUgTw"
      },
      "outputs": [],
      "source": [
        "dataset = df.drop_duplicates() #duplicate values are droped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og4PKcgb9hX3",
        "outputId": "f392cba2-6863-40f3-a7fc-2596b38023a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 49582 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     49582 non-null  object\n",
            " 1   sentiment  49582 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 1.1+ MB\n"
          ]
        }
      ],
      "source": [
        "dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "gYLU8gd9JQel"
      },
      "outputs": [],
      "source": [
        "#In this step we are preparing input for our BERT model\n",
        "# as discussed in model we are adding special tokens and also making all inputs to max length of 256 making all inputs to be same size by adding padding to them if size is less than 256\n",
        "data_bert_input = note.batch_encode_plus(\n",
        "    dataset['review'],\n",
        "    return_tensors='pt',\n",
        "    padding=True,\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    max_length=256,\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpinf-d03ZGk",
        "outputId": "c2628f9c-3554-48d1-ed30-167f306ef58c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "ids = data_bert_input['input_ids']\n",
        "masks = data_bert_input['attention_mask']\n",
        "labels = torch.tensor(dataset['sentiment'].values)\n",
        "\n",
        "tensoredDataset = TensorDataset(ids, masks, labels)\n",
        "# 70% of data for training purpose, 20% testing purpose and 10% for validation\n",
        "train_size = int(0.7 * len(tensoredDataset))\n",
        "test_size =int(0.2 * len(tensoredDataset))\n",
        "val_size = len(tensoredDataset) - train_size - test_size\n",
        "\n",
        "#batch size is kept low because the increase in batch size is crashing the gpu of colab\n",
        "batch_size = 8\n",
        "\n",
        "train_dataset, val_dataset, test_dataset= random_split(tensoredDataset, [train_size, val_size, test_size])\n",
        "# creating dataloaders for our model\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset),batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_dataset, sampler=RandomSampler(val_dataset), batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset,sampler=RandomSampler(test_dataset),batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lExf21O58uze",
        "outputId": "2b2c9174-0b4a-45e4-fd9a-79121b759f64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# BERT model finetuned for sequence classification. Our task is to classify into positive , negative classes so number of output labels for the sequence classification task is 2.\n",
        "# basically we are instantiating bert model for sequence classification\n",
        "Bertimdb = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels = 2, output_attentions = False, output_hidden_states = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "ddBvuY0Q9RGO"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "-Fjm-Kyy6Oqi"
      },
      "outputs": [],
      "source": [
        "#evaluation of the model , here in our code used for the validation dataset.\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    all_predictions = []\n",
        "    all_true_vals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for values in dataloader:\n",
        "            values = tuple(map(lambda x: x.to(device), values))\n",
        "            inputs = {\n",
        "                'input_ids': values[0],\n",
        "                'masks': values[1],\n",
        "                'labels': values[2]\n",
        "            }\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            #onehotencode to convert into required shape of labels for comparision\n",
        "            labels_onehot = torch.nn.functional.one_hot(inputs['labels'], num_classes=2).float()\n",
        "            loss = criterion(logits,labels_onehot)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted_labels = torch.max(outputs.logits, 1)\n",
        "            #checking the correct number of predicted values for calculating accuracy of our model\n",
        "            total_correct += (predicted_labels == inputs['labels']).sum().item()\n",
        "            all_predictions.extend(predicted_labels.tolist())\n",
        "            all_true_vals.extend(inputs['labels'].tolist())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return {\n",
        "        'val_loss': avg_loss,\n",
        "        'predictions': all_predictions,\n",
        "        'true_vals': all_true_vals,\n",
        "        'val_correct_total': total_correct,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "hCMyd_S_STXs"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "optimizer = torch.optim.Adam(Bertimdb.parameters(), lr=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "K5pTm7Ge-FoB"
      },
      "outputs": [],
      "source": [
        "#training block for the model\n",
        "def train_model(Bertimdb,train_dataloader, val_dataloader, epochs, learning_rate):\n",
        "    Bertimdb.to(device)\n",
        "    train_correct_total = 0\n",
        "    train_total_samples = 0\n",
        "    val_correct_total = 0\n",
        "    val_total_samples = 0\n",
        "    trainloss_values=[]\n",
        "    trainaccuracy_values=[]\n",
        "    valloss_values=[]\n",
        "    valaccuracy_values=[]\n",
        "    #to display the output in bars tqdm is used\n",
        "    bars = tqdm(range(1, epochs+1), desc='Epochs', leave=False)\n",
        "    for epoch in bars:\n",
        "        Bertimdb.train()\n",
        "        loss_train_total = 0\n",
        "        for batch_index, batch in enumerate(train_dataloader):\n",
        "            Bertimdb.zero_grad()\n",
        "            batch = tuple(map(lambda b: b.to(device), batch))\n",
        "            inputs = {\n",
        "            'input_ids': batch[0],\n",
        "            'masks': batch[1],\n",
        "            'labels': batch[2]\n",
        "            }\n",
        "            outputs = Bertimdb(**inputs)\n",
        "            logits = outputs.logits\n",
        "            #onehotencode to convert into required shape of labels for comparision\n",
        "            labels_onehot = torch.nn.functional.one_hot(inputs['labels'], num_classes=2).float()\n",
        "\n",
        "            loss = criterion(logits,labels_onehot)\n",
        "            train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(Bertimdb.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            _, predicted_labels = torch.max(outputs.logits, 1)\n",
        "            # predicted values comparision for finding out the correctness of the model\n",
        "            train_correct_total += (predicted_labels == inputs['labels']).sum().item()\n",
        "            train_total_samples += len(inputs['labels'])\n",
        "            #for the display of progress in the output while running the code , it'll help to know and estimate on how much part of data our model has trained and approx time for the completion of training\n",
        "            bars.set_postfix({\n",
        "            'Epoch': epoch,\n",
        "            'Batch': batch_index,\n",
        "            'Training Loss': '{:.3f}'.format(loss.item() / len(batch))\n",
        "            })\n",
        "\n",
        "        avgtrainloss = train_loss / len(train_dataloader)\n",
        "        trainloss_values.append(avgtrainloss)\n",
        "        #validation dataset used to evaluate the model \n",
        "        validation_output = evaluate_model(Bertimdb,val_dataloader)\n",
        "        validationloss = validation_output['val_loss']\n",
        "        predictions = validation_output['predictions']\n",
        "        originalvalues= validation_output['true_vals']\n",
        "        val_correct_total= validation_output['val_correct_total']\n",
        "        originalvals_array = np.array(originalvalues)\n",
        "        predictions_array = np.array(predictions)\n",
        "        f1value = f1_score(originalvals_array.flatten(), np.argmax(predictions_array, axis=1).flatten())\n",
        "        valloss_values.append(validationloss)\n",
        "        train_accuracy = 100* train_correct_total / train_total_samples\n",
        "        validationaccuracy =100* val_correct_total / len(val_dataloader.dataset)\n",
        "        trainaccuracy_values.append(train_accuracy)\n",
        "        valaccuracy_values.append(validationaccuracy)\n",
        "        tqdm.write(f\"|Epoch {epoch}/{epochs} | Train Loss: {avgtrainloss} |Train Accuracy: {train_accuracy} | Val Loss: {validationloss} | Val Accuracy: {validationaccuracy}| F1 score: {f1value}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTj6E54DRedX",
        "outputId": "4567dceb-e734-4756-9f36-e72173ba8a3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:   0%|          | 0/2 [26:28<?, ?it/s, Epoch=1, Batch=4338, Training Loss=0.001]\n",
            "  0%|          | 0/155 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/155 [00:00<01:09,  2.23it/s]\u001b[A\n",
            "  1%|▏         | 2/155 [00:00<01:09,  2.20it/s]\u001b[A\n",
            "  2%|▏         | 3/155 [00:01<01:08,  2.22it/s]\u001b[A\n",
            "  3%|▎         | 4/155 [00:01<01:08,  2.21it/s]\u001b[A\n",
            "  3%|▎         | 5/155 [00:02<01:07,  2.21it/s]\u001b[A\n",
            "  4%|▍         | 6/155 [00:02<01:07,  2.22it/s]\u001b[A\n",
            "  5%|▍         | 7/155 [00:03<01:06,  2.22it/s]\u001b[A\n",
            "  5%|▌         | 8/155 [00:03<01:06,  2.22it/s]\u001b[A\n",
            "  6%|▌         | 9/155 [00:04<01:06,  2.21it/s]\u001b[A\n",
            "  6%|▋         | 10/155 [00:04<01:05,  2.21it/s]\u001b[A\n",
            "  7%|▋         | 11/155 [00:04<01:05,  2.20it/s]\u001b[A\n",
            "  8%|▊         | 12/155 [00:05<01:04,  2.21it/s]\u001b[A\n",
            "  8%|▊         | 13/155 [00:05<01:04,  2.21it/s]\u001b[A\n",
            "  9%|▉         | 14/155 [00:06<01:03,  2.21it/s]\u001b[A\n",
            " 10%|▉         | 15/155 [00:06<01:03,  2.21it/s]\u001b[A\n",
            " 10%|█         | 16/155 [00:07<01:02,  2.21it/s]\u001b[A\n",
            " 11%|█         | 17/155 [00:07<01:02,  2.21it/s]\u001b[A\n",
            " 12%|█▏        | 18/155 [00:08<01:01,  2.21it/s]\u001b[A\n",
            " 12%|█▏        | 19/155 [00:08<01:01,  2.21it/s]\u001b[A\n",
            " 13%|█▎        | 20/155 [00:09<01:01,  2.21it/s]\u001b[A\n",
            " 14%|█▎        | 21/155 [00:09<01:00,  2.20it/s]\u001b[A\n",
            " 14%|█▍        | 22/155 [00:09<01:00,  2.20it/s]\u001b[A\n",
            " 15%|█▍        | 23/155 [00:10<00:59,  2.20it/s]\u001b[A\n",
            " 15%|█▌        | 24/155 [00:10<00:59,  2.20it/s]\u001b[A\n",
            " 16%|█▌        | 25/155 [00:11<00:58,  2.20it/s]\u001b[A\n",
            " 17%|█▋        | 26/155 [00:11<00:58,  2.20it/s]\u001b[A\n",
            " 17%|█▋        | 27/155 [00:12<00:57,  2.21it/s]\u001b[A\n",
            " 18%|█▊        | 28/155 [00:12<00:57,  2.21it/s]\u001b[A\n",
            " 19%|█▊        | 29/155 [00:13<00:57,  2.21it/s]\u001b[A\n",
            " 19%|█▉        | 30/155 [00:13<00:56,  2.21it/s]\u001b[A\n",
            " 20%|██        | 31/155 [00:14<00:56,  2.21it/s]\u001b[A\n",
            " 21%|██        | 32/155 [00:14<00:55,  2.21it/s]\u001b[A\n",
            " 21%|██▏       | 33/155 [00:14<00:55,  2.20it/s]\u001b[A\n",
            " 22%|██▏       | 34/155 [00:15<00:54,  2.21it/s]\u001b[A\n",
            " 23%|██▎       | 35/155 [00:15<00:54,  2.21it/s]\u001b[A\n",
            " 23%|██▎       | 36/155 [00:16<00:53,  2.20it/s]\u001b[A\n",
            " 24%|██▍       | 37/155 [00:16<00:53,  2.20it/s]\u001b[A\n",
            " 25%|██▍       | 38/155 [00:17<00:53,  2.20it/s]\u001b[A\n",
            " 25%|██▌       | 39/155 [00:17<00:52,  2.21it/s]\u001b[A\n",
            " 26%|██▌       | 40/155 [00:18<00:52,  2.20it/s]\u001b[A\n",
            " 26%|██▋       | 41/155 [00:18<00:51,  2.21it/s]\u001b[A\n",
            " 27%|██▋       | 42/155 [00:19<00:51,  2.20it/s]\u001b[A\n",
            " 28%|██▊       | 43/155 [00:19<00:50,  2.21it/s]\u001b[A\n",
            " 28%|██▊       | 44/155 [00:19<00:50,  2.21it/s]\u001b[A\n",
            " 29%|██▉       | 45/155 [00:20<00:49,  2.21it/s]\u001b[A\n",
            " 30%|██▉       | 46/155 [00:20<00:49,  2.21it/s]\u001b[A\n",
            " 30%|███       | 47/155 [00:21<00:48,  2.21it/s]\u001b[A\n",
            " 31%|███       | 48/155 [00:21<00:48,  2.21it/s]\u001b[A\n",
            " 32%|███▏      | 49/155 [00:22<00:48,  2.21it/s]\u001b[A\n",
            " 32%|███▏      | 50/155 [00:22<00:47,  2.20it/s]\u001b[A\n",
            " 33%|███▎      | 51/155 [00:23<00:47,  2.21it/s]\u001b[A\n",
            " 34%|███▎      | 52/155 [00:23<00:46,  2.21it/s]\u001b[A\n",
            " 34%|███▍      | 53/155 [00:24<00:46,  2.20it/s]\u001b[A\n",
            " 35%|███▍      | 54/155 [00:24<00:45,  2.21it/s]\u001b[A\n",
            " 35%|███▌      | 55/155 [00:24<00:45,  2.21it/s]\u001b[A\n",
            " 36%|███▌      | 56/155 [00:25<00:44,  2.21it/s]\u001b[A\n",
            " 37%|███▋      | 57/155 [00:25<00:44,  2.21it/s]\u001b[A\n",
            " 37%|███▋      | 58/155 [00:26<00:44,  2.20it/s]\u001b[A\n",
            " 38%|███▊      | 59/155 [00:26<00:43,  2.21it/s]\u001b[A\n",
            " 39%|███▊      | 60/155 [00:27<00:43,  2.21it/s]\u001b[A\n",
            " 39%|███▉      | 61/155 [00:27<00:42,  2.20it/s]\u001b[A\n",
            " 40%|████      | 62/155 [00:28<00:42,  2.21it/s]\u001b[A\n",
            " 41%|████      | 63/155 [00:28<00:41,  2.20it/s]\u001b[A\n",
            " 41%|████▏     | 64/155 [00:28<00:41,  2.21it/s]\u001b[A\n",
            " 42%|████▏     | 65/155 [00:29<00:40,  2.20it/s]\u001b[A\n",
            " 43%|████▎     | 66/155 [00:29<00:40,  2.21it/s]\u001b[A\n",
            " 43%|████▎     | 67/155 [00:30<00:39,  2.21it/s]\u001b[A\n",
            " 44%|████▍     | 68/155 [00:30<00:39,  2.20it/s]\u001b[A\n",
            " 45%|████▍     | 69/155 [00:31<00:38,  2.21it/s]\u001b[A\n",
            " 45%|████▌     | 70/155 [00:31<00:38,  2.21it/s]\u001b[A\n",
            " 46%|████▌     | 71/155 [00:32<00:37,  2.21it/s]\u001b[A\n",
            " 46%|████▋     | 72/155 [00:32<00:37,  2.21it/s]\u001b[A\n",
            " 47%|████▋     | 73/155 [00:33<00:37,  2.21it/s]\u001b[A\n",
            " 48%|████▊     | 74/155 [00:33<00:36,  2.20it/s]\u001b[A\n",
            " 48%|████▊     | 75/155 [00:33<00:36,  2.21it/s]\u001b[A\n",
            " 49%|████▉     | 76/155 [00:34<00:35,  2.20it/s]\u001b[A\n",
            " 50%|████▉     | 77/155 [00:34<00:35,  2.21it/s]\u001b[A\n",
            " 50%|█████     | 78/155 [00:35<00:34,  2.21it/s]\u001b[A\n",
            " 51%|█████     | 79/155 [00:35<00:34,  2.21it/s]\u001b[A\n",
            " 52%|█████▏    | 80/155 [00:36<00:34,  2.20it/s]\u001b[A\n",
            " 52%|█████▏    | 81/155 [00:36<00:33,  2.20it/s]\u001b[A\n",
            " 53%|█████▎    | 82/155 [00:37<00:33,  2.20it/s]\u001b[A\n",
            " 54%|█████▎    | 83/155 [00:37<00:32,  2.20it/s]\u001b[A\n",
            " 54%|█████▍    | 84/155 [00:38<00:32,  2.21it/s]\u001b[A\n",
            " 55%|█████▍    | 85/155 [00:38<00:31,  2.20it/s]\u001b[A\n",
            " 55%|█████▌    | 86/155 [00:38<00:31,  2.21it/s]\u001b[A\n",
            " 56%|█████▌    | 87/155 [00:39<00:30,  2.20it/s]\u001b[A\n",
            " 57%|█████▋    | 88/155 [00:39<00:30,  2.21it/s]\u001b[A\n",
            " 57%|█████▋    | 89/155 [00:40<00:30,  2.20it/s]\u001b[A\n",
            " 58%|█████▊    | 90/155 [00:40<00:29,  2.20it/s]\u001b[A\n",
            " 59%|█████▊    | 91/155 [00:41<00:29,  2.20it/s]\u001b[A\n",
            " 59%|█████▉    | 92/155 [00:41<00:28,  2.20it/s]\u001b[A\n",
            " 60%|██████    | 93/155 [00:42<00:28,  2.20it/s]\u001b[A\n",
            " 61%|██████    | 94/155 [00:42<00:27,  2.20it/s]\u001b[A\n",
            " 61%|██████▏   | 95/155 [00:43<00:27,  2.19it/s]\u001b[A\n",
            " 62%|██████▏   | 96/155 [00:43<00:26,  2.20it/s]\u001b[A\n",
            " 63%|██████▎   | 97/155 [00:43<00:26,  2.19it/s]\u001b[A\n",
            " 63%|██████▎   | 98/155 [00:44<00:25,  2.20it/s]\u001b[A\n",
            " 64%|██████▍   | 99/155 [00:44<00:25,  2.20it/s]\u001b[A\n",
            " 65%|██████▍   | 100/155 [00:45<00:24,  2.21it/s]\u001b[A\n",
            " 65%|██████▌   | 101/155 [00:45<00:24,  2.20it/s]\u001b[A\n",
            " 66%|██████▌   | 102/155 [00:46<00:24,  2.20it/s]\u001b[A\n",
            " 66%|██████▋   | 103/155 [00:46<00:23,  2.20it/s]\u001b[A\n",
            " 67%|██████▋   | 104/155 [00:47<00:23,  2.21it/s]\u001b[A\n",
            " 68%|██████▊   | 105/155 [00:47<00:22,  2.21it/s]\u001b[A\n",
            " 68%|██████▊   | 106/155 [00:48<00:22,  2.20it/s]\u001b[A\n",
            " 69%|██████▉   | 107/155 [00:48<00:21,  2.21it/s]\u001b[A\n",
            " 70%|██████▉   | 108/155 [00:48<00:21,  2.21it/s]\u001b[A\n",
            " 70%|███████   | 109/155 [00:49<00:20,  2.20it/s]\u001b[A\n",
            " 71%|███████   | 110/155 [00:49<00:20,  2.21it/s]\u001b[A\n",
            " 72%|███████▏  | 111/155 [00:50<00:19,  2.20it/s]\u001b[A\n",
            " 72%|███████▏  | 112/155 [00:50<00:19,  2.20it/s]\u001b[A\n",
            " 73%|███████▎  | 113/155 [00:51<00:19,  2.20it/s]\u001b[A\n",
            " 74%|███████▎  | 114/155 [00:51<00:18,  2.20it/s]\u001b[A\n",
            " 74%|███████▍  | 115/155 [00:52<00:18,  2.20it/s]\u001b[A\n",
            " 75%|███████▍  | 116/155 [00:52<00:17,  2.21it/s]\u001b[A\n",
            " 75%|███████▌  | 117/155 [00:53<00:17,  2.19it/s]\u001b[A\n",
            " 76%|███████▌  | 118/155 [00:53<00:16,  2.20it/s]\u001b[A\n",
            " 77%|███████▋  | 119/155 [00:53<00:16,  2.20it/s]\u001b[A\n",
            " 77%|███████▋  | 120/155 [00:54<00:15,  2.20it/s]\u001b[A\n",
            " 78%|███████▊  | 121/155 [00:54<00:15,  2.20it/s]\u001b[A\n",
            " 79%|███████▊  | 122/155 [00:55<00:14,  2.20it/s]\u001b[A\n",
            " 79%|███████▉  | 123/155 [00:55<00:14,  2.20it/s]\u001b[A\n",
            " 80%|████████  | 124/155 [00:56<00:14,  2.20it/s]\u001b[A\n",
            " 81%|████████  | 125/155 [00:56<00:13,  2.20it/s]\u001b[A\n",
            " 81%|████████▏ | 126/155 [00:57<00:13,  2.21it/s]\u001b[A\n",
            " 82%|████████▏ | 127/155 [00:57<00:12,  2.21it/s]\u001b[A\n",
            " 83%|████████▎ | 128/155 [00:58<00:12,  2.21it/s]\u001b[A\n",
            " 83%|████████▎ | 129/155 [00:58<00:11,  2.20it/s]\u001b[A\n",
            " 84%|████████▍ | 130/155 [00:58<00:11,  2.20it/s]\u001b[A\n",
            " 85%|████████▍ | 131/155 [00:59<00:10,  2.21it/s]\u001b[A\n",
            " 85%|████████▌ | 132/155 [00:59<00:10,  2.20it/s]\u001b[A\n",
            " 86%|████████▌ | 133/155 [01:00<00:09,  2.21it/s]\u001b[A\n",
            " 86%|████████▋ | 134/155 [01:00<00:09,  2.20it/s]\u001b[A\n",
            " 87%|████████▋ | 135/155 [01:01<00:09,  2.21it/s]\u001b[A\n",
            " 88%|████████▊ | 136/155 [01:01<00:08,  2.20it/s]\u001b[A\n",
            " 88%|████████▊ | 137/155 [01:02<00:08,  2.20it/s]\u001b[A\n",
            " 89%|████████▉ | 138/155 [01:02<00:07,  2.20it/s]\u001b[A\n",
            " 90%|████████▉ | 139/155 [01:03<00:07,  2.20it/s]\u001b[A\n",
            " 90%|█████████ | 140/155 [01:03<00:06,  2.21it/s]\u001b[A\n",
            " 91%|█████████ | 141/155 [01:03<00:06,  2.20it/s]\u001b[A\n",
            " 92%|█████████▏| 142/155 [01:04<00:05,  2.21it/s]\u001b[A\n",
            " 92%|█████████▏| 143/155 [01:04<00:05,  2.20it/s]\u001b[A\n",
            " 93%|█████████▎| 144/155 [01:05<00:04,  2.21it/s]\u001b[A\n",
            " 94%|█████████▎| 145/155 [01:05<00:04,  2.20it/s]\u001b[A\n",
            " 94%|█████████▍| 146/155 [01:06<00:04,  2.20it/s]\u001b[A\n",
            " 95%|█████████▍| 147/155 [01:06<00:03,  2.19it/s]\u001b[A\n",
            " 95%|█████████▌| 148/155 [01:07<00:03,  2.20it/s]\u001b[A\n",
            " 96%|█████████▌| 149/155 [01:07<00:02,  2.20it/s]\u001b[A\n",
            " 97%|█████████▋| 150/155 [01:08<00:02,  2.20it/s]\u001b[A\n",
            " 97%|█████████▋| 151/155 [01:08<00:01,  2.20it/s]\u001b[A\n",
            " 98%|█████████▊| 152/155 [01:08<00:01,  2.20it/s]\u001b[A\n",
            " 99%|█████████▊| 153/155 [01:09<00:00,  2.20it/s]\u001b[A\n",
            " 99%|█████████▉| 154/155 [01:09<00:00,  2.20it/s]\u001b[A\n",
            "100%|██████████| 155/155 [01:10<00:00,  2.21it/s]\n",
            "Epochs:  50%|█████     | 1/2 [27:38<27:38, 1658.85s/it, Epoch=1, Batch=4338, Training Loss=0.001]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|Epoch 1/2 | Train Loss: 0.3000410685439844 |Train Accuracy: 89.26729478203244 | Val Loss: 0.29469912809229665 | Val Accuracy: 91.7322040734019| F1 score: 0.9172401934089471\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  50%|█████     | 1/2 [54:08<27:38, 1658.85s/it, Epoch=2, Batch=4338, Training Loss=0.639]\n",
            "  0%|          | 0/155 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/155 [00:00<01:08,  2.26it/s]\u001b[A\n",
            "  1%|▏         | 2/155 [00:00<01:09,  2.21it/s]\u001b[A\n",
            "  2%|▏         | 3/155 [00:01<01:08,  2.23it/s]\u001b[A\n",
            "  3%|▎         | 4/155 [00:01<01:08,  2.22it/s]\u001b[A\n",
            "  3%|▎         | 5/155 [00:02<01:07,  2.22it/s]\u001b[A\n",
            "  4%|▍         | 6/155 [00:02<01:07,  2.22it/s]\u001b[A\n",
            "  5%|▍         | 7/155 [00:03<01:06,  2.21it/s]\u001b[A\n",
            "  5%|▌         | 8/155 [00:03<01:06,  2.22it/s]\u001b[A\n",
            "  6%|▌         | 9/155 [00:04<01:06,  2.21it/s]\u001b[A\n",
            "  6%|▋         | 10/155 [00:04<01:05,  2.22it/s]\u001b[A\n",
            "  7%|▋         | 11/155 [00:04<01:05,  2.21it/s]\u001b[A\n",
            "  8%|▊         | 12/155 [00:05<01:04,  2.21it/s]\u001b[A\n",
            "  8%|▊         | 13/155 [00:05<01:04,  2.21it/s]\u001b[A\n",
            "  9%|▉         | 14/155 [00:06<01:03,  2.20it/s]\u001b[A\n",
            " 10%|▉         | 15/155 [00:06<01:03,  2.21it/s]\u001b[A\n",
            " 10%|█         | 16/155 [00:07<01:02,  2.21it/s]\u001b[A\n",
            " 11%|█         | 17/155 [00:07<01:02,  2.21it/s]\u001b[A\n",
            " 12%|█▏        | 18/155 [00:08<01:01,  2.21it/s]\u001b[A\n",
            " 12%|█▏        | 19/155 [00:08<01:01,  2.21it/s]\u001b[A\n",
            " 13%|█▎        | 20/155 [00:09<01:00,  2.21it/s]\u001b[A\n",
            " 14%|█▎        | 21/155 [00:09<01:00,  2.22it/s]\u001b[A\n",
            " 14%|█▍        | 22/155 [00:09<01:00,  2.21it/s]\u001b[A\n",
            " 15%|█▍        | 23/155 [00:10<00:59,  2.21it/s]\u001b[A\n",
            " 15%|█▌        | 24/155 [00:10<00:59,  2.22it/s]\u001b[A\n",
            " 16%|█▌        | 25/155 [00:11<00:58,  2.21it/s]\u001b[A\n",
            " 17%|█▋        | 26/155 [00:11<00:58,  2.21it/s]\u001b[A\n",
            " 17%|█▋        | 27/155 [00:12<00:57,  2.22it/s]\u001b[A\n",
            " 18%|█▊        | 28/155 [00:12<00:57,  2.22it/s]\u001b[A\n",
            " 19%|█▊        | 29/155 [00:13<00:56,  2.21it/s]\u001b[A\n",
            " 19%|█▉        | 30/155 [00:13<00:56,  2.21it/s]\u001b[A\n",
            " 20%|██        | 31/155 [00:13<00:55,  2.22it/s]\u001b[A\n",
            " 21%|██        | 32/155 [00:14<00:55,  2.21it/s]\u001b[A\n",
            " 21%|██▏       | 33/155 [00:14<00:55,  2.21it/s]\u001b[A\n",
            " 22%|██▏       | 34/155 [00:15<00:54,  2.21it/s]\u001b[A\n",
            " 23%|██▎       | 35/155 [00:15<00:54,  2.22it/s]\u001b[A\n",
            " 23%|██▎       | 36/155 [00:16<00:53,  2.22it/s]\u001b[A\n",
            " 24%|██▍       | 37/155 [00:16<00:53,  2.21it/s]\u001b[A\n",
            " 25%|██▍       | 38/155 [00:17<00:52,  2.22it/s]\u001b[A\n",
            " 25%|██▌       | 39/155 [00:17<00:52,  2.22it/s]\u001b[A\n",
            " 26%|██▌       | 40/155 [00:18<00:51,  2.22it/s]\u001b[A\n",
            " 26%|██▋       | 41/155 [00:18<00:51,  2.22it/s]\u001b[A\n",
            " 27%|██▋       | 42/155 [00:18<00:51,  2.21it/s]\u001b[A\n",
            " 28%|██▊       | 43/155 [00:19<00:50,  2.21it/s]\u001b[A\n",
            " 28%|██▊       | 44/155 [00:19<00:50,  2.21it/s]\u001b[A\n",
            " 29%|██▉       | 45/155 [00:20<00:49,  2.21it/s]\u001b[A\n",
            " 30%|██▉       | 46/155 [00:20<00:49,  2.21it/s]\u001b[A\n",
            " 30%|███       | 47/155 [00:21<00:48,  2.22it/s]\u001b[A\n",
            " 31%|███       | 48/155 [00:21<00:48,  2.22it/s]\u001b[A\n",
            " 32%|███▏      | 49/155 [00:22<00:47,  2.21it/s]\u001b[A\n",
            " 32%|███▏      | 50/155 [00:22<00:47,  2.21it/s]\u001b[A\n",
            " 33%|███▎      | 51/155 [00:23<00:47,  2.21it/s]\u001b[A\n",
            " 34%|███▎      | 52/155 [00:23<00:46,  2.21it/s]\u001b[A\n",
            " 34%|███▍      | 53/155 [00:23<00:46,  2.21it/s]\u001b[A\n",
            " 35%|███▍      | 54/155 [00:24<00:45,  2.21it/s]\u001b[A\n",
            " 35%|███▌      | 55/155 [00:24<00:45,  2.21it/s]\u001b[A\n",
            " 36%|███▌      | 56/155 [00:25<00:44,  2.21it/s]\u001b[A\n",
            " 37%|███▋      | 57/155 [00:25<00:44,  2.21it/s]\u001b[A\n",
            " 37%|███▋      | 58/155 [00:26<00:43,  2.22it/s]\u001b[A\n",
            " 38%|███▊      | 59/155 [00:26<00:43,  2.21it/s]\u001b[A\n",
            " 39%|███▊      | 60/155 [00:27<00:42,  2.21it/s]\u001b[A\n",
            " 39%|███▉      | 61/155 [00:27<00:42,  2.21it/s]\u001b[A\n",
            " 40%|████      | 62/155 [00:27<00:41,  2.22it/s]\u001b[A\n",
            " 41%|████      | 63/155 [00:28<00:41,  2.21it/s]\u001b[A\n",
            " 41%|████▏     | 64/155 [00:28<00:41,  2.22it/s]\u001b[A\n",
            " 42%|████▏     | 65/155 [00:29<00:40,  2.21it/s]\u001b[A\n",
            " 43%|████▎     | 66/155 [00:29<00:40,  2.21it/s]\u001b[A\n",
            " 43%|████▎     | 67/155 [00:30<00:39,  2.21it/s]\u001b[A\n",
            " 44%|████▍     | 68/155 [00:30<00:39,  2.21it/s]\u001b[A\n",
            " 45%|████▍     | 69/155 [00:31<00:38,  2.21it/s]\u001b[A\n",
            " 45%|████▌     | 70/155 [00:31<00:38,  2.21it/s]\u001b[A\n",
            " 46%|████▌     | 71/155 [00:32<00:38,  2.21it/s]\u001b[A\n",
            " 46%|████▋     | 72/155 [00:32<00:37,  2.21it/s]\u001b[A\n",
            " 47%|████▋     | 73/155 [00:32<00:37,  2.21it/s]\u001b[A\n",
            " 48%|████▊     | 74/155 [00:33<00:36,  2.21it/s]\u001b[A\n",
            " 48%|████▊     | 75/155 [00:33<00:36,  2.21it/s]\u001b[A\n",
            " 49%|████▉     | 76/155 [00:34<00:35,  2.21it/s]\u001b[A\n",
            " 50%|████▉     | 77/155 [00:34<00:35,  2.21it/s]\u001b[A\n",
            " 50%|█████     | 78/155 [00:35<00:34,  2.21it/s]\u001b[A\n",
            " 51%|█████     | 79/155 [00:35<00:34,  2.21it/s]\u001b[A\n",
            " 52%|█████▏    | 80/155 [00:36<00:33,  2.21it/s]\u001b[A\n",
            " 52%|█████▏    | 81/155 [00:36<00:33,  2.21it/s]\u001b[A\n",
            " 53%|█████▎    | 82/155 [00:37<00:33,  2.21it/s]\u001b[A\n",
            " 54%|█████▎    | 83/155 [00:37<00:32,  2.21it/s]\u001b[A\n",
            " 54%|█████▍    | 84/155 [00:37<00:32,  2.21it/s]\u001b[A\n",
            " 55%|█████▍    | 85/155 [00:38<00:31,  2.21it/s]\u001b[A\n",
            " 55%|█████▌    | 86/155 [00:38<00:31,  2.21it/s]\u001b[A\n",
            " 56%|█████▌    | 87/155 [00:39<00:30,  2.21it/s]\u001b[A\n",
            " 57%|█████▋    | 88/155 [00:39<00:30,  2.22it/s]\u001b[A\n",
            " 57%|█████▋    | 89/155 [00:40<00:29,  2.21it/s]\u001b[A\n",
            " 58%|█████▊    | 90/155 [00:40<00:29,  2.21it/s]\u001b[A\n",
            " 59%|█████▊    | 91/155 [00:41<00:28,  2.21it/s]\u001b[A\n",
            " 59%|█████▉    | 92/155 [00:41<00:28,  2.21it/s]\u001b[A\n",
            " 60%|██████    | 93/155 [00:42<00:28,  2.21it/s]\u001b[A\n",
            " 61%|██████    | 94/155 [00:42<00:27,  2.21it/s]\u001b[A\n",
            " 61%|██████▏   | 95/155 [00:42<00:27,  2.22it/s]\u001b[A\n",
            " 62%|██████▏   | 96/155 [00:43<00:26,  2.21it/s]\u001b[A\n",
            " 63%|██████▎   | 97/155 [00:43<00:26,  2.21it/s]\u001b[A\n",
            " 63%|██████▎   | 98/155 [00:44<00:25,  2.21it/s]\u001b[A\n",
            " 64%|██████▍   | 99/155 [00:44<00:25,  2.22it/s]\u001b[A\n",
            " 65%|██████▍   | 100/155 [00:45<00:24,  2.21it/s]\u001b[A\n",
            " 65%|██████▌   | 101/155 [00:45<00:24,  2.22it/s]\u001b[A\n",
            " 66%|██████▌   | 102/155 [00:46<00:23,  2.21it/s]\u001b[A\n",
            " 66%|██████▋   | 103/155 [00:46<00:23,  2.21it/s]\u001b[A\n",
            " 67%|██████▋   | 104/155 [00:46<00:23,  2.21it/s]\u001b[A\n",
            " 68%|██████▊   | 105/155 [00:47<00:22,  2.21it/s]\u001b[A\n",
            " 68%|██████▊   | 106/155 [00:47<00:22,  2.21it/s]\u001b[A\n",
            " 69%|██████▉   | 107/155 [00:48<00:21,  2.21it/s]\u001b[A\n",
            " 70%|██████▉   | 108/155 [00:48<00:21,  2.21it/s]\u001b[A\n",
            " 70%|███████   | 109/155 [00:49<00:20,  2.21it/s]\u001b[A\n",
            " 71%|███████   | 110/155 [00:49<00:20,  2.21it/s]\u001b[A\n",
            " 72%|███████▏  | 111/155 [00:50<00:19,  2.21it/s]\u001b[A\n",
            " 72%|███████▏  | 112/155 [00:50<00:19,  2.21it/s]\u001b[A\n",
            " 73%|███████▎  | 113/155 [00:51<00:18,  2.21it/s]\u001b[A\n",
            " 74%|███████▎  | 114/155 [00:51<00:18,  2.21it/s]\u001b[A\n",
            " 74%|███████▍  | 115/155 [00:51<00:18,  2.21it/s]\u001b[A\n",
            " 75%|███████▍  | 116/155 [00:52<00:17,  2.21it/s]\u001b[A\n",
            " 75%|███████▌  | 117/155 [00:52<00:17,  2.21it/s]\u001b[A\n",
            " 76%|███████▌  | 118/155 [00:53<00:16,  2.21it/s]\u001b[A\n",
            " 77%|███████▋  | 119/155 [00:53<00:16,  2.21it/s]\u001b[A\n",
            " 77%|███████▋  | 120/155 [00:54<00:15,  2.21it/s]\u001b[A\n",
            " 78%|███████▊  | 121/155 [00:54<00:15,  2.21it/s]\u001b[A\n",
            " 79%|███████▊  | 122/155 [00:55<00:14,  2.20it/s]\u001b[A\n",
            " 79%|███████▉  | 123/155 [00:55<00:14,  2.21it/s]\u001b[A\n",
            " 80%|████████  | 124/155 [00:56<00:14,  2.21it/s]\u001b[A\n",
            " 81%|████████  | 125/155 [00:56<00:13,  2.21it/s]\u001b[A\n",
            " 81%|████████▏ | 126/155 [00:56<00:13,  2.21it/s]\u001b[A\n",
            " 82%|████████▏ | 127/155 [00:57<00:12,  2.21it/s]\u001b[A\n",
            " 83%|████████▎ | 128/155 [00:57<00:12,  2.21it/s]\u001b[A\n",
            " 83%|████████▎ | 129/155 [00:58<00:11,  2.21it/s]\u001b[A\n",
            " 84%|████████▍ | 130/155 [00:58<00:11,  2.21it/s]\u001b[A\n",
            " 85%|████████▍ | 131/155 [00:59<00:10,  2.21it/s]\u001b[A\n",
            " 85%|████████▌ | 132/155 [00:59<00:10,  2.21it/s]\u001b[A\n",
            " 86%|████████▌ | 133/155 [01:00<00:09,  2.21it/s]\u001b[A\n",
            " 86%|████████▋ | 134/155 [01:00<00:09,  2.21it/s]\u001b[A\n",
            " 87%|████████▋ | 135/155 [01:01<00:09,  2.21it/s]\u001b[A\n",
            " 88%|████████▊ | 136/155 [01:01<00:08,  2.21it/s]\u001b[A\n",
            " 88%|████████▊ | 137/155 [01:01<00:08,  2.21it/s]\u001b[A\n",
            " 89%|████████▉ | 138/155 [01:02<00:07,  2.21it/s]\u001b[A\n",
            " 90%|████████▉ | 139/155 [01:02<00:07,  2.21it/s]\u001b[A\n",
            " 90%|█████████ | 140/155 [01:03<00:06,  2.21it/s]\u001b[A\n",
            " 91%|█████████ | 141/155 [01:03<00:06,  2.21it/s]\u001b[A\n",
            " 92%|█████████▏| 142/155 [01:04<00:05,  2.21it/s]\u001b[A\n",
            " 92%|█████████▏| 143/155 [01:04<00:05,  2.21it/s]\u001b[A\n",
            " 93%|█████████▎| 144/155 [01:05<00:04,  2.21it/s]\u001b[A\n",
            " 94%|█████████▎| 145/155 [01:05<00:04,  2.21it/s]\u001b[A\n",
            " 94%|█████████▍| 146/155 [01:06<00:04,  2.21it/s]\u001b[A\n",
            " 95%|█████████▍| 147/155 [01:06<00:03,  2.21it/s]\u001b[A\n",
            " 95%|█████████▌| 148/155 [01:06<00:03,  2.21it/s]\u001b[A\n",
            " 96%|█████████▌| 149/155 [01:07<00:02,  2.21it/s]\u001b[A\n",
            " 97%|█████████▋| 150/155 [01:07<00:02,  2.21it/s]\u001b[A\n",
            " 97%|█████████▋| 151/155 [01:08<00:01,  2.21it/s]\u001b[A\n",
            " 98%|█████████▊| 152/155 [01:08<00:01,  2.21it/s]\u001b[A\n",
            " 99%|█████████▊| 153/155 [01:09<00:00,  2.21it/s]\u001b[A\n",
            " 99%|█████████▉| 154/155 [01:09<00:00,  2.21it/s]\u001b[A\n",
            "100%|██████████| 155/155 [01:10<00:00,  2.21it/s]\n",
            "                                                                                                 "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|Epoch 2/2 | Train Loss: 0.2052658158826669 |Train Accuracy: 91.88204108681246 | Val Loss: 0.31719807696288393 | Val Accuracy: 92.09518047993546| F1 score: 0.9209309146843441\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "epochs =2\n",
        "\n",
        "train_model(Bertimdb,train_dataloader, val_dataloader, epochs, learning_rate=1e-5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "pN0wM0JWuoAE"
      },
      "outputs": [],
      "source": [
        "#saving the model into pt file so that we can use the trained weights later when testing \n",
        "torch.save(Bertimdb.state_dict(), \"bert.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oiiTfRf3ZGm",
        "outputId": "ae52c08d-97c1-49c4-ec67-1c8b2a64c2c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 310/310 [02:20<00:00,  2.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy :  91.82129891085114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#for testing the unkown dataset or test dataset\n",
        "def test_model(Bertimdb,test_loader):\n",
        "    correct =0\n",
        "    total=0\n",
        "    for batch in tqdm(test_loader):\n",
        "        batch = tuple(map(lambda b: b.to(device), batch))\n",
        "        inputs = {'input_ids': batch[0],'masks': batch[1],'labels': batch[2]}\n",
        "        with torch.no_grad():\n",
        "            outputs = Bertimdb(**inputs)\n",
        "            _, predicted_labels = torch.max(outputs.logits, 1)\n",
        "            correct += (predicted_labels == inputs['labels']).sum().item()\n",
        "            total += len(inputs['labels'])\n",
        "    accuracy=100 * (correct /total)\n",
        "    print('Test Accuracy : ', accuracy)\n",
        "\n",
        "test_model(Bertimdb,test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzNKPD-3xcI0",
        "outputId": "4a413800-d030-4918-c36e-742a9b09f78f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Label: negative\n",
            "Review: My first exposure to the Templarios & not a good one. I was excited to find this title among the offerings from Anchor Bay Video, which has brought us other cult classics such as 'Spider Baby'. The print quality is excellent, but this alone can't hide the fact that the film is deadly dull. There's a thrilling opening sequence in which the villagers exact a terrible revenge on the Templars (& set the whole thing in motion), but everything else in the movie is slow, ponderous &, ultimately, unfulfilling. Adding insult to injury: the movie was dubbed, not subtitled, as promised on the video jacket. \n"
          ]
        }
      ],
      "source": [
        "#using our trained model when new review is passed calculation of its class.\n",
        "def predict_custom_text(text):\n",
        "    inputs = note(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    inputs=inputs.to(device)\n",
        "    Bertimdb.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = Bertimdb(**inputs)\n",
        "    logits = outputs.logits\n",
        "    _, predicted_label = torch.max(logits, dim=1)\n",
        "    class_labels = ['positive','negative'] # used first positive because when we changed our positive, negatives values of dataset in the beginning of our code we assigned 0 to positive and 1 to negative\n",
        "    predicted_class_label = class_labels[predicted_label.item()]\n",
        "    #softmax is used since our model is a classification problem\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "    return predicted_class_label, probabilities\n",
        "\n",
        "custom_text = \"My first exposure to the Templarios & not a good one. I was excited to find this title among the offerings from Anchor Bay Video, which has brought us other cult classics such as 'Spider Baby'. The print quality is excellent, but this alone can't hide the fact that the film is deadly dull. There's a thrilling opening sequence in which the villagers exact a terrible revenge on the Templars (& set the whole thing in motion), but everything else in the movie is slow, ponderous &, ultimately, unfulfilling. Adding insult to injury: the movie was dubbed, not subtitled, as promised on the video jacket. \"\n",
        "predicted_label, probabilities = predict_custom_text(custom_text)\n",
        "print(\"Predicted Label:\", predicted_label)\n",
        "print(\"Review:\",custom_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjZ4JHGP3ZGm"
      },
      "source": [
        "References:\n",
        "1. https://www.analyticsvidhya.com/blog/2021/12/fine-tune-bert-model-for-sentiment-analysis-in-google-colab/\n",
        "2. https://www.kaggle.com/code/satyampd/imdb-sentiment-analysis-using-bert-w-huggingface/notebook\n",
        "3. https://www.kaggle.com/code/chayan8/sentiment-analysis-using-bert-pytorch"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "history_visible": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
